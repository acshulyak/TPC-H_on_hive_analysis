
Logging initialized using configuration in jar:file:/home/lca/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/lca/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/lca/tez/apache-tez-0.7.0-src/tez_minimal_jars/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Exception in thread "main" java.lang.RuntimeException: java.net.ConnectException: Call From jupiter3.ece.utexas.edu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Call From jupiter3.ece.utexas.edu/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy15.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1424)
	at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:596)
	at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:554)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:508)
	... 8 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 28 more
Command exited with non-zero status 1
Time:12.70

Logging initialized using configuration in jar:file:/home/lca/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/lca/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/lca/tez/apache-tez-0.7.0-src/tez_minimal_jars/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
DROP TABLE orders
OK
Time taken: 2.179 seconds

DROP TABLE lineitem
OK
Time taken: 0.104 seconds

DROP TABLE customer
OK
Time taken: 0.091 seconds

DROP TABLE q3_shipping_priority
OK
Time taken: 0.096 seconds


Create external table lineitem (L_ORDERKEY INT, L_PARTKEY INT, L_SUPPKEY INT, L_LINENUMBER INT, L_QUANTITY DOUBLE, L_EXTENDEDPRICE DOUBLE, L_DISCOUNT DOUBLE, L_TAX DOUBLE, L_RETURNFLAG STRING, L_LINESTATUS STRING, L_SHIPDATE STRING, L_COMMITDATE STRING, L_RECEIPTDATE STRING, L_SHIPINSTRUCT STRING, L_SHIPMODE STRING, L_COMMENT STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tpch/lineitem'
OK
Time taken: 0.564 seconds

create external table orders (O_ORDERKEY INT, O_CUSTKEY INT, O_ORDERSTATUS STRING, O_TOTALPRICE DOUBLE, O_ORDERDATE STRING, O_ORDERPRIORITY STRING, O_CLERK STRING, O_SHIPPRIORITY INT, O_COMMENT STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tpch/orders'
OK
Time taken: 0.048 seconds

create external table customer (C_CUSTKEY INT, C_NAME STRING, C_ADDRESS STRING, C_NATIONKEY INT, C_PHONE STRING, C_ACCTBAL DOUBLE, C_MKTSEGMENT STRING, C_COMMENT STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/tpch/customer'
OK
Time taken: 0.042 seconds


create table q3_shipping_priority (l_orderkey int, revenue double, o_orderdate string, o_shippriority int)
OK
Time taken: 0.133 seconds
set mapred.min.split.size=536870912
set hive.exec.reducers.bytes.per.reducer=1024000000


Insert overwrite table q3_shipping_priority 
select 
  l_orderkey, sum(l_extendedprice*(1-l_discount)) as revenue, o_orderdate, o_shippriority 
from 
  customer c join orders o 
    on c.c_mktsegment = 'BUILDING' and c.c_custkey = o.o_custkey 
  join lineitem l 
    on l.l_orderkey = o.o_orderkey
where 
  o_orderdate < '1995-03-15' and l_shipdate > '1995-03-15' 
group by l_orderkey, o_orderdate, o_shippriority 
order by revenue desc, o_orderdate 
limit 10
Query ID = lca_20160107134716_3c77ea77-dd5c-4cc0-a9e0-b27c993fa69c
Total jobs = 6
Stage-1 is selected by condition resolver.
Launching Job 1 out of 6
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1452189223910_0003, Tracking URL = http://jupiter3.ece.utexas.edu:8088/proxy/application_1452189223910_0003/
Kill Command = /home/lca/hadoop/bin/hadoop job  -kill job_1452189223910_0003
Hadoop job information for Stage-1: number of mappers: 8; number of reducers: 2
2016-01-07 13:47:25,304 Stage-1 map = 0%,  reduce = 0%
2016-01-07 13:47:42,342 Stage-1 map = 25%,  reduce = 0%, Cumulative CPU 61.75 sec
2016-01-07 13:47:45,506 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 81.21 sec
2016-01-07 13:47:46,544 Stage-1 map = 58%,  reduce = 0%, Cumulative CPU 84.35 sec
2016-01-07 13:47:47,588 Stage-1 map = 75%,  reduce = 0%, Cumulative CPU 91.73 sec
2016-01-07 13:47:55,906 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 107.84 sec
2016-01-07 13:47:59,013 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 115.19 sec
2016-01-07 13:48:00,046 Stage-1 map = 100%,  reduce = 69%, Cumulative CPU 124.29 sec
2016-01-07 13:48:01,086 Stage-1 map = 100%,  reduce = 78%, Cumulative CPU 128.25 sec
2016-01-07 13:48:03,145 Stage-1 map = 100%,  reduce = 87%, Cumulative CPU 131.75 sec
2016-01-07 13:48:04,174 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 136.35 sec
MapReduce Total cumulative CPU time: 2 minutes 16 seconds 350 msec
Ended Job = job_1452189223910_0003
Stage-14 is filtered out by condition resolver.
Stage-15 is filtered out by condition resolver.
Stage-2 is selected by condition resolver.
Launching Job 2 out of 6
Number of reduce tasks not specified. Estimated from input data size: 8
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1452189223910_0004, Tracking URL = http://jupiter3.ece.utexas.edu:8088/proxy/application_1452189223910_0004/
Kill Command = /home/lca/hadoop/bin/hadoop job  -kill job_1452189223910_0004
Hadoop job information for Stage-2: number of mappers: 30; number of reducers: 8
2016-01-07 13:48:14,965 Stage-2 map = 0%,  reduce = 0%
2016-01-07 13:48:31,864 Stage-2 map = 7%,  reduce = 0%, Cumulative CPU 62.52 sec
2016-01-07 13:48:35,024 Stage-2 map = 14%,  reduce = 0%, Cumulative CPU 82.8 sec
2016-01-07 13:48:36,060 Stage-2 map = 19%,  reduce = 0%, Cumulative CPU 89.18 sec
2016-01-07 13:48:37,125 Stage-2 map = 20%,  reduce = 0%, Cumulative CPU 90.98 sec
2016-01-07 13:48:49,858 Stage-2 map = 22%,  reduce = 0%, Cumulative CPU 140.53 sec
2016-01-07 13:48:50,899 Stage-2 map = 24%,  reduce = 0%, Cumulative CPU 147.01 sec
2016-01-07 13:48:51,938 Stage-2 map = 28%,  reduce = 0%, Cumulative CPU 153.52 sec
2016-01-07 13:48:52,978 Stage-2 map = 29%,  reduce = 0%, Cumulative CPU 159.98 sec
2016-01-07 13:48:54,026 Stage-2 map = 32%,  reduce = 0%, Cumulative CPU 168.59 sec
2016-01-07 13:48:55,070 Stage-2 map = 39%,  reduce = 0%, Cumulative CPU 180.2 sec
2016-01-07 13:48:56,120 Stage-2 map = 40%,  reduce = 0%, Cumulative CPU 181.01 sec
2016-01-07 13:49:05,568 Stage-2 map = 42%,  reduce = 2%, Cumulative CPU 196.97 sec
2016-01-07 13:49:07,651 Stage-2 map = 44%,  reduce = 3%, Cumulative CPU 213.83 sec
2016-01-07 13:49:08,692 Stage-2 map = 47%,  reduce = 3%, Cumulative CPU 220.69 sec
2016-01-07 13:49:09,729 Stage-2 map = 51%,  reduce = 3%, Cumulative CPU 229.17 sec
2016-01-07 13:49:10,775 Stage-2 map = 53%,  reduce = 4%, Cumulative CPU 231.06 sec
2016-01-07 13:49:19,158 Stage-2 map = 54%,  reduce = 4%, Cumulative CPU 239.64 sec
2016-01-07 13:49:20,238 Stage-2 map = 56%,  reduce = 4%, Cumulative CPU 247.81 sec
2016-01-07 13:49:21,278 Stage-2 map = 58%,  reduce = 4%, Cumulative CPU 264.06 sec
2016-01-07 13:49:22,314 Stage-2 map = 59%,  reduce = 4%, Cumulative CPU 267.39 sec
2016-01-07 13:49:23,374 Stage-2 map = 62%,  reduce = 5%, Cumulative CPU 275.6 sec
2016-01-07 13:49:24,398 Stage-2 map = 64%,  reduce = 5%, Cumulative CPU 279.59 sec
2016-01-07 13:49:25,447 Stage-2 map = 66%,  reduce = 5%, Cumulative CPU 281.51 sec
2016-01-07 13:49:26,488 Stage-2 map = 67%,  reduce = 5%, Cumulative CPU 283.79 sec
2016-01-07 13:49:29,597 Stage-2 map = 67%,  reduce = 6%, Cumulative CPU 283.87 sec
2016-01-07 13:49:33,744 Stage-2 map = 67%,  reduce = 8%, Cumulative CPU 284.91 sec
2016-01-07 13:49:35,802 Stage-2 map = 69%,  reduce = 8%, Cumulative CPU 301.58 sec
2016-01-07 13:49:36,838 Stage-2 map = 70%,  reduce = 8%, Cumulative CPU 309.78 sec
2016-01-07 13:49:37,861 Stage-2 map = 72%,  reduce = 8%, Cumulative CPU 313.25 sec
2016-01-07 13:49:38,907 Stage-2 map = 77%,  reduce = 8%, Cumulative CPU 320.66 sec
2016-01-07 13:49:39,943 Stage-2 map = 77%,  reduce = 9%, Cumulative CPU 321.97 sec
2016-01-07 13:49:43,041 Stage-2 map = 77%,  reduce = 10%, Cumulative CPU 328.71 sec
2016-01-07 13:49:48,202 Stage-2 map = 78%,  reduce = 10%, Cumulative CPU 337.74 sec
2016-01-07 13:49:49,226 Stage-2 map = 79%,  reduce = 10%, Cumulative CPU 345.97 sec
2016-01-07 13:49:50,258 Stage-2 map = 80%,  reduce = 10%, Cumulative CPU 354.21 sec
2016-01-07 13:49:51,282 Stage-2 map = 82%,  reduce = 10%, Cumulative CPU 358.01 sec
2016-01-07 13:49:52,308 Stage-2 map = 84%,  reduce = 10%, Cumulative CPU 361.29 sec
2016-01-07 13:49:53,341 Stage-2 map = 87%,  reduce = 10%, Cumulative CPU 364.68 sec
2016-01-07 13:49:54,373 Stage-2 map = 87%,  reduce = 11%, Cumulative CPU 364.77 sec
2016-01-07 13:50:00,573 Stage-2 map = 88%,  reduce = 11%, Cumulative CPU 373.2 sec
2016-01-07 13:50:01,603 Stage-2 map = 89%,  reduce = 11%, Cumulative CPU 381.5 sec
2016-01-07 13:50:03,662 Stage-2 map = 91%,  reduce = 11%, Cumulative CPU 385.38 sec
2016-01-07 13:50:04,685 Stage-2 map = 94%,  reduce = 11%, Cumulative CPU 396.92 sec
2016-01-07 13:50:06,744 Stage-2 map = 94%,  reduce = 12%, Cumulative CPU 397.21 sec
2016-01-07 13:50:07,782 Stage-2 map = 97%,  reduce = 12%, Cumulative CPU 401.18 sec
2016-01-07 13:50:13,960 Stage-2 map = 98%,  reduce = 12%, Cumulative CPU 409.12 sec
2016-01-07 13:50:16,006 Stage-2 map = 98%,  reduce = 16%, Cumulative CPU 413.45 sec
2016-01-07 13:50:17,028 Stage-2 map = 99%,  reduce = 16%, Cumulative CPU 416.75 sec
2016-01-07 13:50:18,051 Stage-2 map = 99%,  reduce = 20%, Cumulative CPU 420.62 sec
2016-01-07 13:50:19,086 Stage-2 map = 100%,  reduce = 20%, Cumulative CPU 422.51 sec
2016-01-07 13:50:20,147 Stage-2 map = 100%,  reduce = 22%, Cumulative CPU 423.09 sec
2016-01-07 13:50:21,190 Stage-2 map = 100%,  reduce = 31%, Cumulative CPU 427.69 sec
2016-01-07 13:50:22,232 Stage-2 map = 100%,  reduce = 39%, Cumulative CPU 434.81 sec
2016-01-07 13:50:23,272 Stage-2 map = 100%,  reduce = 42%, Cumulative CPU 438.31 sec
2016-01-07 13:50:24,310 Stage-2 map = 100%,  reduce = 44%, Cumulative CPU 449.21 sec
2016-01-07 13:50:26,377 Stage-2 map = 100%,  reduce = 45%, Cumulative CPU 456.29 sec
2016-01-07 13:50:27,421 Stage-2 map = 100%,  reduce = 49%, Cumulative CPU 466.1 sec
2016-01-07 13:50:28,462 Stage-2 map = 100%,  reduce = 51%, Cumulative CPU 472.63 sec
2016-01-07 13:50:29,495 Stage-2 map = 100%,  reduce = 52%, Cumulative CPU 475.85 sec
2016-01-07 13:50:30,528 Stage-2 map = 100%,  reduce = 55%, Cumulative CPU 482.23 sec
2016-01-07 13:50:31,579 Stage-2 map = 100%,  reduce = 66%, Cumulative CPU 495.73 sec
2016-01-07 13:50:32,621 Stage-2 map = 100%,  reduce = 68%, Cumulative CPU 501.59 sec
2016-01-07 13:50:33,673 Stage-2 map = 100%,  reduce = 69%, Cumulative CPU 505.55 sec
2016-01-07 13:50:34,713 Stage-2 map = 100%,  reduce = 71%, Cumulative CPU 515.94 sec
2016-01-07 13:50:35,749 Stage-2 map = 100%,  reduce = 72%, Cumulative CPU 518.08 sec
2016-01-07 13:50:36,783 Stage-2 map = 100%,  reduce = 74%, Cumulative CPU 521.46 sec
2016-01-07 13:50:39,856 Stage-2 map = 100%,  reduce = 75%, Cumulative CPU 524.98 sec
2016-01-07 13:50:42,949 Stage-2 map = 100%,  reduce = 84%, Cumulative CPU 533.05 sec
2016-01-07 13:50:43,971 Stage-2 map = 100%,  reduce = 92%, Cumulative CPU 541.21 sec
2016-01-07 13:50:46,024 Stage-2 map = 100%,  reduce = 94%, Cumulative CPU 545.13 sec
2016-01-07 13:50:47,056 Stage-2 map = 100%,  reduce = 96%, Cumulative CPU 548.81 sec
2016-01-07 13:50:49,102 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 557.07 sec
MapReduce Total cumulative CPU time: 9 minutes 17 seconds 70 msec
Ended Job = job_1452189223910_0004
Launching Job 3 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1452189223910_0005, Tracking URL = http://jupiter3.ece.utexas.edu:8088/proxy/application_1452189223910_0005/
Kill Command = /home/lca/hadoop/bin/hadoop job  -kill job_1452189223910_0005
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2016-01-07 13:50:59,859 Stage-3 map = 0%,  reduce = 0%
2016-01-07 13:51:07,063 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 3.81 sec
2016-01-07 13:51:13,253 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 7.44 sec
MapReduce Total cumulative CPU time: 7 seconds 440 msec
Ended Job = job_1452189223910_0005
Launching Job 4 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1452189223910_0006, Tracking URL = http://jupiter3.ece.utexas.edu:8088/proxy/application_1452189223910_0006/
Kill Command = /home/lca/hadoop/bin/hadoop job  -kill job_1452189223910_0006
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2016-01-07 13:51:24,016 Stage-4 map = 0%,  reduce = 0%
2016-01-07 13:51:30,206 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 3.4 sec
2016-01-07 13:51:36,419 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 5.61 sec
MapReduce Total cumulative CPU time: 5 seconds 610 msec
Ended Job = job_1452189223910_0006
Loading data to table default.q3_shipping_priority
Table default.q3_shipping_priority stats: [numFiles=1, numRows=10, totalSize=362, rawDataSize=352]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 8  Reduce: 2   Cumulative CPU: 136.35 sec   HDFS Read: 1994158831 HDFS Write: 49783581 SUCCESS
Stage-Stage-2: Map: 30  Reduce: 8   Cumulative CPU: 557.48 sec   HDFS Read: 7825980007 HDFS Write: 4804309 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 7.44 sec   HDFS Read: 4811604 HDFS Write: 4803577 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 5.61 sec   HDFS Read: 4809105 HDFS Write: 447 SUCCESS
Total MapReduce CPU Time Spent: 11 minutes 46 seconds 880 msec
OK
Time taken: 261.377 seconds
Time:272.29
